# ДЗ 3
- Задача - Множественная классификация
- Датасет - CIFAR-10 (целиком)
- Архитектура - EfficientNet
- Гиперпараметры:
  - оптимайзер: Adam 
  - lr: 0.001, scheduler каждые 10 эпох умножает на 0.4.
  - epoch: 50
  - batch: 64

### Результаты обучения
Все графики можно посмотреть в wandb:\
https://wandb.ai/kavesnin-hse/Homework-1.

Сохранял в тот же wandb, что и в первом дз. \
Запуск претрейна (предобучение features) - ssl_pretraining. \
- training_loss: 1.470
- val_loss: 1.482

Затем провёл 3 эксперимента, количество датасета: 100%, 50%, 10%.
Для каждого эксперимента по 2 запуска: scratch_N - обучение с нуля, 
ssl_N - обучение с features из pretrain-а.

В запусках с N=50 и 100 получилось так, что после первого эпохи лосс
 при предобучении был ниже, чем лосс модели со случайными features:
1. N=100: ssl_val_loss: 2.04, scratch_val_loss: 2.40
2. N=50: ssl_val_loss: 2.12, scratch_val_loss: 2.67

Однако в конечном итоге для этих запусков модель со случайными features сошлась лучше (см метрики wandb)

Вывод: такое поведение контр-интуитивно, я считаю, что для запуска ssl нужно было
подобрать другие гиперпараметры, и тогда мы получили бы лучшее схождение или такое же.
Оно не может быть хуже, если на первой эпохе модель в более оптимальном
 состоянии.

Для N=10 эффект ssl удалось полностью продемонстрировать, например, финальные:
1. ssl_f1_avg = 0.36
2. scratch_f1_avg = 0.29