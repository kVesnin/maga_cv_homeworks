# ДЗ 4
- Задача - Множественная классификация
- Датасет - CIFAR-10 (целиком)
- Архитектура:
  - EfficientNet-bo - для студента (4M)
  - EfficientNet-b2 - для учителя (9M)
- Гиперпараметры:
  - оптимайзер: Adam 
  - lr: 0.001, scheduler каждые 2 эпохи умножает на 0.5.
  - epoch: 10
  - batch: 64

### Результаты обучения
Все графики можно посмотреть в wandb:\
https://wandb.ai/kavesnin-hse/Homework-4.

Запуски проводил на 10 эпох для более быстрого результата.

Запуски на обучении логитов и скрытому вектору прошли гладко, однако для регрессора я выставил
коэффициенты loss-ов MSE и CrossEntropy 0.5 и 0.5. Получился плохой результат, 
поэтому я дополнительно провёл эксперимент с коэффициентом MSE = 0.1.

Сравнение финальных метрик на примере f1_avg (другие метрики можно просмотреть в wandb):
  - no_distil = 0.46
  - distil_logits = 0.49
  - distil_hidden = 0.47
  - distil_regressor_0.5 = 0.33
  - distil_regressor_0.1 = 0.45
  - teacher = 0.66


Из интересных наблюдений loss для MSE сильно выше, чем loss
для CrossEntropy, а loss для cosine сильно ниже, поэтому нужно быть аккуратным при сравнении обучений
по loss-у. Чтобы нивилировать этот фактор, я решил test_loss мерить чисто
по CrossEntropy без MSE и cosine.

Выводы: 
1. Дистилляция по логитам дала результат, и качество метрик стало выше
по сравнению с тем, если учить с нуля. 
2. Дистилляция с регрессором ничего не дала, возможно, она требует ещё более тонкой настройки. 
3. Дистилляция по cosine на features также дала небольшой эффект по сравнению с отсутствием дистилляции, однако дистилляция чисто по logit-ом у меня перформит лучше.
4. Для более наглядного эффекта, вероятно, следовало взять архитектуру студента с меньшим количеством параметров.
